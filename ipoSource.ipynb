{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1880872-020e-4a53-9366-40545c63ab4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: jugaad-data in /Users/reva/Library/Python/3.9/lib/python/site-packages (0.27)\n",
      "Requirement already satisfied: requests in /Users/reva/Library/Python/3.9/lib/python/site-packages (from jugaad-data) (2.32.3)\n",
      "Requirement already satisfied: click==7.1.2 in /Users/reva/Library/Python/3.9/lib/python/site-packages (from jugaad-data) (7.1.2)\n",
      "Requirement already satisfied: appdirs==1.4.4 in /Users/reva/Library/Python/3.9/lib/python/site-packages (from jugaad-data) (1.4.4)\n",
      "Requirement already satisfied: beautifulsoup4==4.9.3 in /Users/reva/Library/Python/3.9/lib/python/site-packages (from jugaad-data) (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/reva/Library/Python/3.9/lib/python/site-packages (from beautifulsoup4==4.9.3->jugaad-data) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/reva/Library/Python/3.9/lib/python/site-packages (from requests->jugaad-data) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/reva/Library/Python/3.9/lib/python/site-packages (from requests->jugaad-data) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/reva/Library/Python/3.9/lib/python/site-packages (from requests->jugaad-data) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/reva/Library/Python/3.9/lib/python/site-packages (from requests->jugaad-data) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install jugaad-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c0184a10-6ff6-45f5-9d5e-0a57da6d0d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from jugaad_data.nse import stock_df\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ca20b8f7-4a53-4579-a91f-6f882ae17af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"README.md\"\n",
    "images_dir = \"images\"\n",
    "\n",
    "# Create the images directory if it doesn't exist\n",
    "if not os.path.exists(images_dir):\n",
    "    os.makedirs(images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "995546ef-7fc0-41d5-a779-23c6cce314bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQ symbols saved to 'eq_symbols.csv'\n"
     ]
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('IPO_16To25.csv')\n",
    "\n",
    "# Filter rows where the SECURITY TYPE is \"EQ\"\n",
    "eq_symbols_df = df[df['SECURITY TYPE'] == 'EQ']\n",
    "\n",
    "# Extract the Symbol column\n",
    "symbols = eq_symbols_df['Symbol'].tolist()\n",
    "\n",
    "# Save the symbols to a new CSV file\n",
    "symbols_df = pd.DataFrame(symbols, columns=['Symbol'])\n",
    "symbols_df.to_csv('eq_symbols.csv', index=False)\n",
    "\n",
    "print(\"EQ symbols saved to 'eq_symbols.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "eb949cd6-b8ce-47f3-baad-3169b7ddb06a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of stocks listed from the year 2016 of type EQ :  336\n",
      "['LAXMIDENTL', 'QUADFUTURE', 'SGLTL', 'INDOFARM', 'UNIMECH', 'VENTIVE', 'CARRARO', 'SENORES', 'TRANSRAILL', 'SANATHAN', 'CEWATER', 'DAMCAPITAL', 'IGIL', 'IKS', 'VMM', 'SAILIFE', 'MOBIKWIK', 'SURAKSHA', 'EIEL', 'NTPCGREEN', 'BLACKBUCK', 'NIVABUPA', 'SWIGGY', 'ACMESOLAR', 'SAGILITY', 'AFCONS', 'GODAVARIB', 'WAAREEENER', 'DBEIL', 'HYUNDAI', 'GARUDA', 'KRN', 'ARKADE', 'NORTHARC', 'WCIL', 'PNGJL', 'KROSS', 'BAJAJHFL', 'STYLEBAAZA', 'ECOSMOBLTY', 'PREMIERENE', 'INTERARCH', 'FIRSTCRY', 'UNIECOM', 'OLAELEC', 'CEIGALL', 'AKUMS', 'SANSTAR', 'EMCURE', 'BANSALWIRE', 'ABDL', 'STANLEY', 'DEEDEV', 'IXIGO', 'KRONOX', 'AWFIS', 'GODIGIT', 'TBOTEK', 'AADHARHFC', 'INDGN', 'JNKINDIA', 'IDEAFPO', 'BHARTIHEXA', 'KRYSTAL', 'PVSL', 'GOPAL', 'JGCHEM', 'RKSWAMY', 'EXICOM', 'GPTHEALTH', 'JUNIPER', 'ENTERO', 'CAPITALSFB', 'JSFB', 'RPTECH', 'PARKHOTELS', 'BLSE', 'EPACK', 'MEDIASSIST', 'JYOTICNC', 'INNOVACAP', 'AZAD', 'HAPPYFORGE', 'MUFTI', 'MUTHOOTMF', 'SURAJEST', 'INOXINDIA', 'DOMS', 'INDIASHLTR', 'FLAIR', 'TATATECH', 'GANDHAR', 'FEDFINA', 'ASKAUTOLTD', 'PROTEAN', 'ESAFSFB', 'HONASA', 'CELLO', 'BLUEJET', 'IRMENERGY', 'UDS', 'JSWINFRA', 'MVGJL', 'SIGNATURE', 'KALAMANDIR', 'YATRA', 'ZAGGLE', 'SAMHI', 'RRKABEL', 'EMSLIMITED', 'JLHL', 'RISHABH', 'VPRPL', 'AEROFLEX', 'TVSSCS', 'CONCORDBIO', 'SBFC', 'YATHARTH', 'NETWEB', 'UTKARSHBNK', 'SENCO', 'PKH', 'PKH', 'CYIENTDLM', 'IDEAFORGE', 'HMAAGRO', 'IKIO', 'MANKIND', 'AVALON', 'DIVGIITTS', 'ADANIENTPP', 'RADIANTCMS', 'ELIN', 'KFINTECH', 'LANDMARK', 'SULA', 'AHL', 'UNIPARTS', 'DHARMAJ', 'RUSTOMJEE', 'INOXGREEN', 'KAYNES', 'ACI', 'FIVESTAR', 'BIKAJI', 'MEDANTA', 'FUSION', 'DCX', 'TRACXN', 'EMIL', 'HARSHA', 'DREAMFOLKS', 'SYRMA', 'AETHER', 'EMUDHRA', 'ETHOSLTD', 'PARADEEP', 'DELHIVERY', 'VENUSPIPES', 'PRUDENT', 'LICI', 'RAINBOW', 'CAMPUS', 'RUCHISOYA', 'MANYAVAR', 'AWL', 'AGSTRA', 'CMSINFO', 'SUPRIYA', 'DATAPATTNS', 'MEDPLUS', 'METROBRAND', 'MAPMYINDIA', 'SHRIRAMPPS', 'RATEGAIN', 'ANANDRATHI', 'TEGA', 'STARHEALTH', 'GOCOLORS', 'TARSONS', 'LATENTVIEW', 'SAPPHIRE', 'PAYTM', 'SJS', 'POLICYBZR', 'FINOPB', 'NYKAA', 'ABSLAMC', 'SANSERA', 'VIJAYA', 'AMIORG', 'CHEMPLASTS', 'APTUS', 'NUVOCO', 'CARTRADE', 'DEVYANI', 'WINDLAS', 'KRSNAA', 'ROLEXRINGS', 'GLS', 'TATVA', 'ZOMATO', 'GRINFRA', 'CLEAN', 'IPL', 'DODLA', 'KIMS', 'SONACOMS', 'SHYAMMETL', 'LODHA', 'BARBEQUE', 'NAZARA', 'SURYODAY', 'KALYANKJIL', 'LXCHEM', 'CRAFTSMAN', 'EASEMYTRIP', 'MTARTECH', 'HERANBA', 'RAILTEL', 'STOVEKRAFT', 'HOMEFIRST', 'INDIGOPNTS', 'AWHCL', 'BECTORFOOD', 'BURGERKING', 'GLAND', 'EQUITASBNK', 'UTIAMC', 'MAZDOCK', 'ANGELBRKG', 'CHEMCON', 'CAMS', 'ROUTE', 'HAPPSTMNDS', 'YESBANK', 'ROSSARI', 'AWHCL', 'SBICARD', 'ITI', 'PRINCEPIPE', 'UJJIVANSFB', 'CSBBANK', 'IRCTC', 'SWSL', 'SPANDANA', 'AFFLE', 'KPRAGRO', 'INDIAMART', 'POLYCAB', 'METROPOLIS', 'RVNL', 'MSTCLTD', 'CHALET', 'DEPL', 'AAVAS', 'GRSE', 'IRCON', 'HDFCAMC', 'TCNSBRANDS', 'VARROC', 'FINEORG', 'RITES', 'INDOSTAR', 'LEMONTREE', 'ISEC', 'MIDHANI', 'SANDHAR', 'HAL', 'KARDA', 'BANDHANBNK', 'BDL', 'HGINFRA', 'ASTERDM', 'GALAXYSURF', 'AMBER', 'NEWGEN', 'ASTRON', 'FSC', 'SHALBY', 'HDFCLIFE', 'KHADIM', 'NIACL', 'MAHLOG', 'RNAM', 'GICRE', 'IEX', 'MASFIN', 'GODREJAGRO', 'DIAMONDYD', 'SBILIFE', 'ICICIGI', 'CAPACITE', 'MATRIMONY', 'BRNL', 'DIXON', 'APEX', 'COCHINSHIP', 'SIS', 'AUBANK', 'GTPL', 'CDSL', 'ERIS', 'TEJASNET', 'HUDCO', 'SCHAND', 'SHANKARA', 'DMART', 'BSE', 'LAURUSLABS', 'SFL', 'GSBPL', 'VBL', 'PNBHOUSING', 'ENDURANCE', 'HPL', 'ICICIPRULI', 'GNA', 'LTTS', 'RBLBANK', 'SPAL', 'DBL', 'ADVENZYMES', 'LTI', 'QUESS', 'MGL', 'PARAGMILK', 'UJJIVAN', 'THYROCARE', 'EQUITAS', 'INFIBEAM', 'BHARATWIRE', 'HCG', 'QUICKHEAL', 'TEAMLEASE', 'PRECAM']\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of stocks listed from the year 2016 of type EQ : \", len(symbols))\n",
    "print(symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed76f2-e41c-435a-84dd-3e436a03aa5b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File for LAXMIDENTL already exists. Skipping...\n",
      "File for QUADFUTURE already exists. Skipping...\n",
      "File for SGLTL already exists. Skipping...\n",
      "File for INDOFARM already exists. Skipping...\n",
      "File for UNIMECH already exists. Skipping...\n",
      "File for VENTIVE already exists. Skipping...\n",
      "File for CARRARO already exists. Skipping...\n",
      "File for SENORES already exists. Skipping...\n",
      "File for TRANSRAILL already exists. Skipping...\n",
      "File for SANATHAN already exists. Skipping...\n",
      "File for CEWATER already exists. Skipping...\n",
      "File for DAMCAPITAL already exists. Skipping...\n",
      "File for IGIL already exists. Skipping...\n",
      "File for IKS already exists. Skipping...\n",
      "File for VMM already exists. Skipping...\n",
      "File for SAILIFE already exists. Skipping...\n",
      "File for MOBIKWIK already exists. Skipping...\n",
      "File for SURAKSHA already exists. Skipping...\n",
      "File for EIEL already exists. Skipping...\n",
      "File for NTPCGREEN already exists. Skipping...\n",
      "File for BLACKBUCK already exists. Skipping...\n",
      "File for NIVABUPA already exists. Skipping...\n",
      "File for SWIGGY already exists. Skipping...\n",
      "File for ACMESOLAR already exists. Skipping...\n",
      "File for SAGILITY already exists. Skipping...\n",
      "File for AFCONS already exists. Skipping...\n",
      "File for GODAVARIB already exists. Skipping...\n",
      "File for WAAREEENER already exists. Skipping...\n",
      "File for DBEIL already exists. Skipping...\n",
      "File for HYUNDAI already exists. Skipping...\n",
      "File for GARUDA already exists. Skipping...\n",
      "File for KRN already exists. Skipping...\n",
      "File for ARKADE already exists. Skipping...\n",
      "File for NORTHARC already exists. Skipping...\n",
      "File for WCIL already exists. Skipping...\n",
      "File for PNGJL already exists. Skipping...\n",
      "File for KROSS already exists. Skipping...\n",
      "File for BAJAJHFL already exists. Skipping...\n",
      "File for STYLEBAAZA already exists. Skipping...\n",
      "File for ECOSMOBLTY already exists. Skipping...\n",
      "File for PREMIERENE already exists. Skipping...\n",
      "File for INTERARCH already exists. Skipping...\n",
      "File for FIRSTCRY already exists. Skipping...\n",
      "File for UNIECOM already exists. Skipping...\n",
      "File for OLAELEC already exists. Skipping...\n",
      "File for CEIGALL already exists. Skipping...\n",
      "File for AKUMS already exists. Skipping...\n",
      "File for SANSTAR already exists. Skipping...\n",
      "File for EMCURE already exists. Skipping...\n",
      "File for BANSALWIRE already exists. Skipping...\n",
      "File for ABDL already exists. Skipping...\n",
      "File for STANLEY already exists. Skipping...\n",
      "File for DEEDEV already exists. Skipping...\n",
      "File for IXIGO already exists. Skipping...\n",
      "File for KRONOX already exists. Skipping...\n",
      "File for AWFIS already exists. Skipping...\n",
      "File for GODIGIT already exists. Skipping...\n",
      "File for TBOTEK already exists. Skipping...\n",
      "File for AADHARHFC already exists. Skipping...\n",
      "File for INDGN already exists. Skipping...\n",
      "File for JNKINDIA already exists. Skipping...\n",
      "Fetching data for IDEAFPO...\n",
      "Error fetching data for IDEAFPO: \"None of [Index(['CH_TIMESTAMP', 'CH_SERIES', 'CH_OPENING_PRICE', 'CH_TRADE_HIGH_PRICE',\\n       'CH_TRADE_LOW_PRICE', 'CH_PREVIOUS_CLS_PRICE', 'CH_LAST_TRADED_PRICE',\\n       'CH_CLOSING_PRICE', 'VWAP', 'CH_52WEEK_HIGH_PRICE',\\n       'CH_52WEEK_LOW_PRICE', 'CH_TOT_TRADED_QTY', 'CH_TOT_TRADED_VAL',\\n       'CH_TOTAL_TRADES', 'CH_SYMBOL'],\\n      dtype='object')] are in the [columns]\"\n",
      "File for BHARTIHEXA already exists. Skipping...\n",
      "File for KRYSTAL already exists. Skipping...\n",
      "File for PVSL already exists. Skipping...\n",
      "File for GOPAL already exists. Skipping...\n",
      "File for JGCHEM already exists. Skipping...\n",
      "File for RKSWAMY already exists. Skipping...\n",
      "File for EXICOM already exists. Skipping...\n",
      "File for GPTHEALTH already exists. Skipping...\n",
      "File for JUNIPER already exists. Skipping...\n",
      "File for ENTERO already exists. Skipping...\n",
      "File for CAPITALSFB already exists. Skipping...\n",
      "File for JSFB already exists. Skipping...\n",
      "File for RPTECH already exists. Skipping...\n",
      "File for PARKHOTELS already exists. Skipping...\n",
      "File for BLSE already exists. Skipping...\n",
      "File for EPACK already exists. Skipping...\n",
      "File for MEDIASSIST already exists. Skipping...\n",
      "File for JYOTICNC already exists. Skipping...\n",
      "File for INNOVACAP already exists. Skipping...\n",
      "File for AZAD already exists. Skipping...\n",
      "File for HAPPYFORGE already exists. Skipping...\n",
      "File for MUFTI already exists. Skipping...\n",
      "File for MUTHOOTMF already exists. Skipping...\n",
      "File for SURAJEST already exists. Skipping...\n",
      "File for INOXINDIA already exists. Skipping...\n",
      "File for DOMS already exists. Skipping...\n",
      "File for INDIASHLTR already exists. Skipping...\n",
      "File for FLAIR already exists. Skipping...\n",
      "File for TATATECH already exists. Skipping...\n",
      "File for GANDHAR already exists. Skipping...\n",
      "File for FEDFINA already exists. Skipping...\n",
      "File for ASKAUTOLTD already exists. Skipping...\n",
      "Fetching data for PROTEAN...\n",
      "Error fetching data for PROTEAN: \"None of [Index(['CH_TIMESTAMP', 'CH_SERIES', 'CH_OPENING_PRICE', 'CH_TRADE_HIGH_PRICE',\\n       'CH_TRADE_LOW_PRICE', 'CH_PREVIOUS_CLS_PRICE', 'CH_LAST_TRADED_PRICE',\\n       'CH_CLOSING_PRICE', 'VWAP', 'CH_52WEEK_HIGH_PRICE',\\n       'CH_52WEEK_LOW_PRICE', 'CH_TOT_TRADED_QTY', 'CH_TOT_TRADED_VAL',\\n       'CH_TOTAL_TRADES', 'CH_SYMBOL'],\\n      dtype='object')] are in the [columns]\"\n",
      "File for ESAFSFB already exists. Skipping...\n",
      "File for HONASA already exists. Skipping...\n",
      "File for CELLO already exists. Skipping...\n",
      "File for BLUEJET already exists. Skipping...\n",
      "File for IRMENERGY already exists. Skipping...\n",
      "File for UDS already exists. Skipping...\n",
      "File for JSWINFRA already exists. Skipping...\n",
      "File for MVGJL already exists. Skipping...\n",
      "File for SIGNATURE already exists. Skipping...\n",
      "File for KALAMANDIR already exists. Skipping...\n",
      "File for YATRA already exists. Skipping...\n",
      "File for ZAGGLE already exists. Skipping...\n",
      "File for SAMHI already exists. Skipping...\n",
      "File for RRKABEL already exists. Skipping...\n",
      "File for EMSLIMITED already exists. Skipping...\n",
      "File for JLHL already exists. Skipping...\n",
      "File for RISHABH already exists. Skipping...\n",
      "File for VPRPL already exists. Skipping...\n",
      "File for AEROFLEX already exists. Skipping...\n",
      "File for TVSSCS already exists. Skipping...\n",
      "File for CONCORDBIO already exists. Skipping...\n",
      "File for SBFC already exists. Skipping...\n",
      "File for YATHARTH already exists. Skipping...\n",
      "File for NETWEB already exists. Skipping...\n",
      "File for UTKARSHBNK already exists. Skipping...\n",
      "File for SENCO already exists. Skipping...\n",
      "Fetching data for PKH...\n",
      "Error fetching data for PKH: \"None of [Index(['CH_TIMESTAMP', 'CH_SERIES', 'CH_OPENING_PRICE', 'CH_TRADE_HIGH_PRICE',\\n       'CH_TRADE_LOW_PRICE', 'CH_PREVIOUS_CLS_PRICE', 'CH_LAST_TRADED_PRICE',\\n       'CH_CLOSING_PRICE', 'VWAP', 'CH_52WEEK_HIGH_PRICE',\\n       'CH_52WEEK_LOW_PRICE', 'CH_TOT_TRADED_QTY', 'CH_TOT_TRADED_VAL',\\n       'CH_TOTAL_TRADES', 'CH_SYMBOL'],\\n      dtype='object')] are in the [columns]\"\n",
      "Fetching data for PKH...\n",
      "Error fetching data for PKH: \"None of [Index(['CH_TIMESTAMP', 'CH_SERIES', 'CH_OPENING_PRICE', 'CH_TRADE_HIGH_PRICE',\\n       'CH_TRADE_LOW_PRICE', 'CH_PREVIOUS_CLS_PRICE', 'CH_LAST_TRADED_PRICE',\\n       'CH_CLOSING_PRICE', 'VWAP', 'CH_52WEEK_HIGH_PRICE',\\n       'CH_52WEEK_LOW_PRICE', 'CH_TOT_TRADED_QTY', 'CH_TOT_TRADED_VAL',\\n       'CH_TOTAL_TRADES', 'CH_SYMBOL'],\\n      dtype='object')] are in the [columns]\"\n",
      "File for CYIENTDLM already exists. Skipping...\n",
      "File for IDEAFORGE already exists. Skipping...\n",
      "File for HMAAGRO already exists. Skipping...\n",
      "File for IKIO already exists. Skipping...\n",
      "File for MANKIND already exists. Skipping...\n",
      "File for AVALON already exists. Skipping...\n",
      "File for DIVGIITTS already exists. Skipping...\n",
      "Fetching data for ADANIENTPP...\n",
      "Error fetching data for ADANIENTPP: \"None of [Index(['CH_TIMESTAMP', 'CH_SERIES', 'CH_OPENING_PRICE', 'CH_TRADE_HIGH_PRICE',\\n       'CH_TRADE_LOW_PRICE', 'CH_PREVIOUS_CLS_PRICE', 'CH_LAST_TRADED_PRICE',\\n       'CH_CLOSING_PRICE', 'VWAP', 'CH_52WEEK_HIGH_PRICE',\\n       'CH_52WEEK_LOW_PRICE', 'CH_TOT_TRADED_QTY', 'CH_TOT_TRADED_VAL',\\n       'CH_TOTAL_TRADES', 'CH_SYMBOL'],\\n      dtype='object')] are in the [columns]\"\n",
      "File for RADIANTCMS already exists. Skipping...\n",
      "File for ELIN already exists. Skipping...\n",
      "File for KFINTECH already exists. Skipping...\n",
      "File for LANDMARK already exists. Skipping...\n",
      "File for SULA already exists. Skipping...\n",
      "File for AHL already exists. Skipping...\n",
      "File for UNIPARTS already exists. Skipping...\n",
      "File for DHARMAJ already exists. Skipping...\n",
      "File for RUSTOMJEE already exists. Skipping...\n",
      "File for INOXGREEN already exists. Skipping...\n",
      "File for KAYNES already exists. Skipping...\n",
      "File for ACI already exists. Skipping...\n",
      "File for FIVESTAR already exists. Skipping...\n",
      "File for BIKAJI already exists. Skipping...\n",
      "File for MEDANTA already exists. Skipping...\n",
      "File for FUSION already exists. Skipping...\n",
      "Fetching data for DCX...\n",
      "Error fetching data for DCX: \"None of [Index(['CH_TIMESTAMP', 'CH_SERIES', 'CH_OPENING_PRICE', 'CH_TRADE_HIGH_PRICE',\\n       'CH_TRADE_LOW_PRICE', 'CH_PREVIOUS_CLS_PRICE', 'CH_LAST_TRADED_PRICE',\\n       'CH_CLOSING_PRICE', 'VWAP', 'CH_52WEEK_HIGH_PRICE',\\n       'CH_52WEEK_LOW_PRICE', 'CH_TOT_TRADED_QTY', 'CH_TOT_TRADED_VAL',\\n       'CH_TOTAL_TRADES', 'CH_SYMBOL'],\\n      dtype='object')] are in the [columns]\"\n",
      "File for TRACXN already exists. Skipping...\n",
      "File for EMIL already exists. Skipping...\n",
      "File for HARSHA already exists. Skipping...\n",
      "File for DREAMFOLKS already exists. Skipping...\n",
      "File for SYRMA already exists. Skipping...\n",
      "File for AETHER already exists. Skipping...\n",
      "File for EMUDHRA already exists. Skipping...\n",
      "File for ETHOSLTD already exists. Skipping...\n",
      "File for PARADEEP already exists. Skipping...\n",
      "File for DELHIVERY already exists. Skipping...\n",
      "File for VENUSPIPES already exists. Skipping...\n",
      "File for PRUDENT already exists. Skipping...\n",
      "File for LICI already exists. Skipping...\n",
      "File for RAINBOW already exists. Skipping...\n",
      "File for CAMPUS already exists. Skipping...\n",
      "File for RUCHISOYA already exists. Skipping...\n",
      "File for MANYAVAR already exists. Skipping...\n",
      "File for AWL already exists. Skipping...\n",
      "File for AGSTRA already exists. Skipping...\n",
      "File for CMSINFO already exists. Skipping...\n",
      "File for SUPRIYA already exists. Skipping...\n",
      "File for DATAPATTNS already exists. Skipping...\n",
      "File for MEDPLUS already exists. Skipping...\n",
      "File for METROBRAND already exists. Skipping...\n",
      "File for MAPMYINDIA already exists. Skipping...\n",
      "File for SHRIRAMPPS already exists. Skipping...\n",
      "File for RATEGAIN already exists. Skipping...\n",
      "File for ANANDRATHI already exists. Skipping...\n",
      "File for TEGA already exists. Skipping...\n",
      "File for STARHEALTH already exists. Skipping...\n",
      "File for GOCOLORS already exists. Skipping...\n",
      "File for TARSONS already exists. Skipping...\n",
      "File for LATENTVIEW already exists. Skipping...\n",
      "File for SAPPHIRE already exists. Skipping...\n",
      "File for PAYTM already exists. Skipping...\n",
      "File for SJS already exists. Skipping...\n",
      "File for POLICYBZR already exists. Skipping...\n",
      "File for FINOPB already exists. Skipping...\n",
      "File for NYKAA already exists. Skipping...\n",
      "File for ABSLAMC already exists. Skipping...\n",
      "File for SANSERA already exists. Skipping...\n",
      "File for VIJAYA already exists. Skipping...\n",
      "File for AMIORG already exists. Skipping...\n",
      "File for CHEMPLASTS already exists. Skipping...\n",
      "File for APTUS already exists. Skipping...\n",
      "File for NUVOCO already exists. Skipping...\n",
      "File for CARTRADE already exists. Skipping...\n",
      "File for DEVYANI already exists. Skipping...\n",
      "File for WINDLAS already exists. Skipping...\n",
      "File for KRSNAA already exists. Skipping...\n",
      "File for ROLEXRINGS already exists. Skipping...\n",
      "File for GLS already exists. Skipping...\n",
      "File for TATVA already exists. Skipping...\n",
      "File for ZOMATO already exists. Skipping...\n",
      "File for GRINFRA already exists. Skipping...\n",
      "File for CLEAN already exists. Skipping...\n",
      "File for IPL already exists. Skipping...\n",
      "File for DODLA already exists. Skipping...\n",
      "File for KIMS already exists. Skipping...\n",
      "File for SONACOMS already exists. Skipping...\n",
      "File for SHYAMMETL already exists. Skipping...\n",
      "File for LODHA already exists. Skipping...\n",
      "File for BARBEQUE already exists. Skipping...\n",
      "File for NAZARA already exists. Skipping...\n",
      "File for SURYODAY already exists. Skipping...\n",
      "File for KALYANKJIL already exists. Skipping...\n",
      "File for LXCHEM already exists. Skipping...\n",
      "File for CRAFTSMAN already exists. Skipping...\n",
      "File for EASEMYTRIP already exists. Skipping...\n",
      "File for MTARTECH already exists. Skipping...\n",
      "File for HERANBA already exists. Skipping...\n",
      "File for RAILTEL already exists. Skipping...\n",
      "File for STOVEKRAFT already exists. Skipping...\n",
      "File for HOMEFIRST already exists. Skipping...\n",
      "File for INDIGOPNTS already exists. Skipping...\n",
      "File for AWHCL already exists. Skipping...\n",
      "File for BECTORFOOD already exists. Skipping...\n",
      "File for BURGERKING already exists. Skipping...\n",
      "File for GLAND already exists. Skipping...\n",
      "File for EQUITASBNK already exists. Skipping...\n",
      "File for UTIAMC already exists. Skipping...\n",
      "File for MAZDOCK already exists. Skipping...\n",
      "File for ANGELBRKG already exists. Skipping...\n",
      "File for CHEMCON already exists. Skipping...\n",
      "File for CAMS already exists. Skipping...\n",
      "File for ROUTE already exists. Skipping...\n",
      "File for HAPPSTMNDS already exists. Skipping...\n",
      "File for YESBANK already exists. Skipping...\n",
      "File for ROSSARI already exists. Skipping...\n",
      "File for AWHCL already exists. Skipping...\n",
      "File for SBICARD already exists. Skipping...\n",
      "File for ITI already exists. Skipping...\n",
      "File for PRINCEPIPE already exists. Skipping...\n",
      "File for UJJIVANSFB already exists. Skipping...\n",
      "File for CSBBANK already exists. Skipping...\n",
      "File for IRCTC already exists. Skipping...\n"
     ]
    }
   ],
   "source": [
    "# Create the \"data\" directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Dates for fetching data\n",
    "from_date = date(2016, 1, 1)\n",
    "to_date = date(2025, 1, 25)\n",
    "\n",
    "# Record the total start time\n",
    "total_start_time = time.time()\n",
    "\n",
    "# Download stock data and save it to CSV\n",
    "for symbol in symbols:\n",
    "    csv_file_path = f\"data/{symbol}.csv\"\n",
    "    if os.path.exists(csv_file_path):\n",
    "        print(f\"File for {symbol} already exists. Skipping...\")\n",
    "        continue  # Skip to the next symbol\n",
    "\n",
    "    # Add a delay of 2 to 5 seconds between calls - Else gets throttled by NSE\n",
    "    time.sleep(random.uniform(2, 5))\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()  # Record the start time\n",
    "\n",
    "        # Fetch stock data\n",
    "        print(f\"Fetching data for {symbol}...\")\n",
    "        df = stock_df(symbol=symbol, \n",
    "                      from_date=from_date, \n",
    "                      to_date=to_date, \n",
    "                      series=\"EQ\")\n",
    "\n",
    "        # Save to CSV\n",
    "        df.to_csv(csv_file_path, index=False)\n",
    "        save_time = time.time()  # Record the time after saving\n",
    "\n",
    "        print(f\"Total time taken for {symbol}: {save_time - start_time:.2f} seconds\\n\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching data for {symbol}: {e}\")\n",
    "\n",
    "# Record the total end time\n",
    "total_end_time = time.time()\n",
    "print(f\"Total time taken for all symbols: {total_end_time - total_start_time:.2f} seconds\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae328f4-06b4-47f3-a53b-2c2fc66e303c",
   "metadata": {},
   "source": [
    "#### Distribution of Lifetime IPO returns from the date launched till date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f282e75f-7301-4a1d-b944-237eee1e27f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing CSV files\n",
    "data_dir = \"data/\"\n",
    "\n",
    "# List to store return percentages\n",
    "returns = []\n",
    "\n",
    "# Loop through all files in the \"data\" directory\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Extract stock name from the filename (excluding the .csv extension)\n",
    "        stock_name = filename.replace(\".csv\", \"\")\n",
    "        \n",
    "        # Load the stock data from the CSV file\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Get the oldest and latest closing prices\n",
    "        oldest_close = df['CLOSE'].iloc[-1]  # Oldest price (last row)\n",
    "        latest_close = df['CLOSE'].iloc[0]  # Latest price (first row)\n",
    "        \n",
    "        # Calculate the return percentage\n",
    "        return_percentage = ((latest_close - oldest_close) / oldest_close) * 100\n",
    "        \n",
    "        # Store the return percentage in the list\n",
    "        returns.append(return_percentage)\n",
    "\n",
    "# Clean the README.md file by opening it in write mode\n",
    "with open(output_file, \"w\") as readme:\n",
    "    readme.write(\"# Stock Performance Analysis\\n\")\n",
    "    readme.write(\"This report categorizes stocks by IPO age and displays those with a CAGR of >=20%.\\n\\n\")\n",
    "    \n",
    "    # Iterate through your categories and plot the first set of histograms\n",
    "    for idx, (category, returns) in enumerate(returns_by_category.items(), 1):\n",
    "        plt.subplot(2, 2, idx)\n",
    "        \n",
    "        # Calculate mean, median, and variance\n",
    "        mean_return = np.mean(returns)\n",
    "        median_return = np.median(returns)\n",
    "        variance_return = np.var(returns)\n",
    "        \n",
    "        # Plot the histogram\n",
    "        n, bins, patches = plt.hist(returns, bins=30, color='skyblue', edgecolor='black')\n",
    "        \n",
    "        # Annotate counts on top of each bar\n",
    "        for i in range(len(patches)):\n",
    "            count = int(n[i])  # Get the count for each bar\n",
    "            if count > 0:  # Only annotate bars with counts > 0\n",
    "                plt.text(patches[i].get_x() + patches[i].get_width() / 2, \n",
    "                         n[i], \n",
    "                         str(count), \n",
    "                         ha='center', \n",
    "                         va='bottom', \n",
    "                         fontsize=8, \n",
    "                         color='black')\n",
    "\n",
    "        # Add annotations for mean, median, and variance in the top-right corner\n",
    "        plt.axvline(mean_return, color='red', linestyle='dashed', linewidth=1)\n",
    "        plt.axvline(median_return, color='green', linestyle='dashed', linewidth=1)\n",
    "        \n",
    "        plt.xlabel('Return Percentage')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'Return Distribution for {category}')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.text(0.95, 0.95, f'Mean: {mean_return:.2f}%', ha='right', va='top', transform=plt.gca().transAxes, color=\"red\")\n",
    "        plt.text(0.95, 0.90, f'Median: {median_return:.2f}%', ha='right', va='top', transform=plt.gca().transAxes, color=\"green\")\n",
    "        plt.text(0.95, 0.85, f'Variance: {variance_return:.2f}', ha='right', va='top', transform=plt.gca().transAxes, color=\"blue\")\n",
    "        \n",
    "        plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "        \n",
    "        # Save the plot to the images folder\n",
    "        plot_filename = f\"{images_dir}/return_distribution_{category.replace(' ', '_')}.png\"\n",
    "        plt.savefig(plot_filename, format=\"png\")\n",
    "        \n",
    "        # Write analysis text to README.md\n",
    "        readme.write(f\"## {category} - Return Distribution\\n\")\n",
    "        readme.write(f\"Mean Return: {mean_return:.2f}%\\n\")\n",
    "        readme.write(f\"Median Return: {median_return:.2f}%\\n\")\n",
    "        readme.write(f\"Variance: {variance_return:.2f}\\n\\n\")\n",
    "        readme.write(f\"![Return Distribution for {category}](./{plot_filename})\\n\\n\")\n",
    "        \n",
    "        # Clear the plot to avoid overlapping between subplots\n",
    "        plt.clf()\n",
    "\n",
    "    # Now handle the second plot block for the distribution of stock returns\n",
    "    # Compute statistics\n",
    "    mean_return = np.mean(returns)\n",
    "    median_return = np.median(returns)\n",
    "    variance_return = np.var(returns)\n",
    "\n",
    "    # Plot the distribution of stock returns with counts on top of each bar\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    counts, bins, patches = plt.hist(returns, bins=30, color='skyblue', edgecolor='black')\n",
    "\n",
    "    # Add counts on top of the histogram bars\n",
    "    bin_centers = 0.5 * (bins[:-1] + bins[1:])  # Compute the center of each bin\n",
    "    for count, center in zip(counts, bin_centers):\n",
    "        plt.text(center, count, str(int(count)), ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "    # Add statistics in the top-right corner\n",
    "    stats_text = (\n",
    "        f\"Mean: {mean_return:.2f}\\n\"\n",
    "        f\"Median: {median_return:.2f}\\n\"\n",
    "        f\"Variance: {variance_return:.2f}\"\n",
    "    )\n",
    "    plt.text(\n",
    "        0.95, 0.95, stats_text, transform=plt.gca().transAxes, fontsize=10,\n",
    "        verticalalignment='top', horizontalalignment='right', bbox=dict(facecolor='white', alpha=0.8)\n",
    "    )\n",
    "\n",
    "    plt.xlabel('Return Percentage')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Distribution of Stock Returns')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "\n",
    "    # Save the plot to the images folder\n",
    "    plot_filename = f\"{images_dir}/distribution_of_stock_returns.png\"\n",
    "    plt.savefig(plot_filename, format=\"png\")\n",
    "\n",
    "    # Write analysis text to README.md\n",
    "    readme.write(f\"## Distribution of Stock Returns\\n\")\n",
    "    readme.write(f\"Mean Return: {mean_return:.2f}%\\n\")\n",
    "    readme.write(f\"Median Return: {median_return:.2f}%\\n\")\n",
    "    readme.write(f\"Variance: {variance_return:.2f}\\n\\n\")\n",
    "    readme.write(f\"![Distribution of Stock Returns](./{plot_filename})\\n\\n\")\n",
    "    \n",
    "    # Clear the plot to avoid any potential overlaps in further plotting\n",
    "    plt.clf()\n",
    "\n",
    "print(f\"Results and plots have been written to {output_file} and images are stored in the '{images_dir}' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faac35b-19d1-44cd-b988-ac94bc05b62a",
   "metadata": {},
   "source": [
    "#### Distribution of IPO stocks with their returns divided based on their age/term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "50b235ad-c2e3-44ed-8dc9-be09c17f37b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results and plots have been written to README.md.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Path to the directory containing CSV files\n",
    "data_dir = \"data/\"\n",
    "\n",
    "# Dictionary to store return percentages for each year category\n",
    "returns_by_category = {\n",
    "    \"<=1 year\": [],\n",
    "    \">1 and <=2 years\": [],\n",
    "    \">2 and <=5 years\": [],\n",
    "    \">5 and <=10 years\": []\n",
    "}\n",
    "\n",
    "# Loop through all files in the \"data\" directory\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Extract stock name from the filename (excluding the .csv extension)\n",
    "        stock_name = filename.replace(\".csv\", \"\")\n",
    "        \n",
    "        # Load the stock data from the CSV file\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Parse the oldest and latest dates\n",
    "        oldest_date = datetime.strptime(df['DATE'].iloc[-1], \"%Y-%m-%d\")\n",
    "        latest_date = datetime.strptime(df['DATE'].iloc[0], \"%Y-%m-%d\")\n",
    "        \n",
    "        # Calculate the number of years between the oldest and latest date\n",
    "        years_diff = (latest_date - oldest_date).days / 365.25\n",
    "        \n",
    "        # Calculate the return percentage\n",
    "        oldest_close = df['CLOSE'].iloc[-1]  # Oldest price (last row)\n",
    "        latest_close = df['CLOSE'].iloc[0]  # Latest price (first row)\n",
    "        return_percentage = ((latest_close - oldest_close) / oldest_close) * 100\n",
    "        \n",
    "        # Assign the return percentage to the appropriate year category\n",
    "        if years_diff <= 1:\n",
    "            returns_by_category[\"<=1 year\"].append(return_percentage)\n",
    "        elif 1 < years_diff <= 2:\n",
    "            returns_by_category[\">1 and <=2 years\"].append(return_percentage)\n",
    "        elif 2 < years_diff <= 5:\n",
    "            returns_by_category[\">2 and <=5 years\"].append(return_percentage)\n",
    "        elif 5 < years_diff <= 10:\n",
    "            returns_by_category[\">5 and <=10 years\"].append(return_percentage)\n",
    "\n",
    "# Plot the distribution of returns for each year category\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Clean the README.md file by opening it in write mode\n",
    "with open(output_file, \"a\") as readme:\n",
    "    readme.write(\"# Stock Performance Analysis\\n\")\n",
    "    readme.write(\"This report categorizes stocks by IPO age and displays those with a CAGR of >=20%.\\n\\n\")\n",
    "    \n",
    "    # Create subplots for each category\n",
    "    for idx, (category, returns) in enumerate(returns_by_category.items(), 1):\n",
    "        plt.subplot(2, 2, idx)\n",
    "        \n",
    "        # Calculate mean, median, and variance\n",
    "        mean_return = np.mean(returns)\n",
    "        median_return = np.median(returns)\n",
    "        variance_return = np.var(returns)\n",
    "        \n",
    "        # Plot the histogram\n",
    "        n, bins, patches = plt.hist(returns, bins=30, color='skyblue', edgecolor='black')\n",
    "        \n",
    "        # Annotate counts on top of each bar\n",
    "        for i in range(len(patches)):\n",
    "            count = int(n[i])  # Get the count for each bar\n",
    "            if count > 0:  # Only annotate bars with counts > 0\n",
    "                plt.text(patches[i].get_x() + patches[i].get_width() / 2, \n",
    "                         n[i], \n",
    "                         str(count), \n",
    "                         ha='center', \n",
    "                         va='bottom', \n",
    "                         fontsize=8, \n",
    "                         color='black')\n",
    "\n",
    "        # Add annotations for mean, median, and variance in the top-right corner\n",
    "        plt.axvline(mean_return, color='red', linestyle='dashed', linewidth=1)\n",
    "        plt.axvline(median_return, color='green', linestyle='dashed', linewidth=1)\n",
    "        \n",
    "        plt.xlabel('Return Percentage')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'Return Distribution for {category}')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.text(0.95, 0.95, f'Mean: {mean_return:.2f}%', ha='right', va='top', transform=plt.gca().transAxes, color=\"red\")\n",
    "        plt.text(0.95, 0.90, f'Median: {median_return:.2f}%', ha='right', va='top', transform=plt.gca().transAxes, color=\"green\")\n",
    "        plt.text(0.95, 0.85, f'Variance: {variance_return:.2f}', ha='right', va='top', transform=plt.gca().transAxes, color=\"blue\")\n",
    "        \n",
    "        plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "        \n",
    "        # Save the plot to a PNG file\n",
    "        plot_filename = f\"{images_dir}/return_distribution_{category.replace(' ', '_')}.png\"\n",
    "        plt.savefig(plot_filename, format=\"png\")\n",
    "        \n",
    "        # Write analysis text to README.md\n",
    "        readme.write(f\"## {category} - Return Distribution\\n\")\n",
    "        readme.write(f\"Mean Return: {mean_return:.2f}%\\n\")\n",
    "        readme.write(f\"Median Return: {median_return:.2f}%\\n\")\n",
    "        readme.write(f\"Variance: {variance_return:.2f}\\n\\n\")\n",
    "        readme.write(f\"![Return Distribution for {category}](./{plot_filename})\\n\\n\")\n",
    "        \n",
    "        # Clear the plot to avoid overlapping between subplots\n",
    "        plt.clf()\n",
    "\n",
    "print(f\"Results and plots have been written to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07e6031-7085-434f-99b9-dcb9b28fbc4c",
   "metadata": {},
   "source": [
    "#### CAGR Distribution in different categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fb1e1796-571f-404f-923a-4a3d658046f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results and plots have been written to README.md and images are stored in the 'images' folder.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Path to the directory containing CSV files\n",
    "data_dir = \"data/\"\n",
    "\n",
    "# Dictionary to store CAGR values for each year category\n",
    "cagr_by_category = {\n",
    "    \"<=1 year\": [],\n",
    "    \">1 and <=2 years\": [],\n",
    "    \">2 and <=5 years\": [],\n",
    "    \">5 and <=10 years\": []\n",
    "}\n",
    "\n",
    "# Loop through all files in the \"data\" directory\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Extract stock name from the filename (excluding the .csv extension)\n",
    "        stock_name = filename.replace(\".csv\", \"\")\n",
    "        \n",
    "        # Load the stock data from the CSV file\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Parse the oldest and latest dates\n",
    "        oldest_date = datetime.strptime(df['DATE'].iloc[-1], \"%Y-%m-%d\")\n",
    "        latest_date = datetime.strptime(df['DATE'].iloc[0], \"%Y-%m-%d\")\n",
    "        \n",
    "        # Calculate the number of years between the oldest and latest date\n",
    "        years_diff = (latest_date - oldest_date).days / 365.25\n",
    "        \n",
    "        # Calculate CAGR\n",
    "        oldest_close = df['CLOSE'].iloc[-1]  # Oldest price (last row)\n",
    "        latest_close = df['CLOSE'].iloc[0]  # Latest price (first row)\n",
    "        if years_diff > 0:\n",
    "            cagr = ((latest_close / oldest_close) ** (1 / years_diff)) - 1\n",
    "            cagr *= 100  # Convert to percentage\n",
    "            \n",
    "            # Assign the CAGR to the appropriate year category\n",
    "            if years_diff <= 1:\n",
    "                cagr_by_category[\"<=1 year\"].append(cagr)\n",
    "            elif 1 < years_diff <= 2:\n",
    "                cagr_by_category[\">1 and <=2 years\"].append(cagr)\n",
    "            elif 2 < years_diff <= 5:\n",
    "                cagr_by_category[\">2 and <=5 years\"].append(cagr)\n",
    "            elif 5 < years_diff <= 10:\n",
    "                cagr_by_category[\">5 and <=10 years\"].append(cagr)\n",
    "\n",
    "# Plot the distribution of CAGR for each year category\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Clean the README.md file by opening it in write mode\n",
    "with open(output_file, \"a\") as readme:\n",
    "    readme.write(\"# Stock Performance Analysis\\n\")\n",
    "    readme.write(\"This report categorizes stocks by IPO age and displays those with a CAGR of >=20%.\\n\\n\")\n",
    "    \n",
    "    # Iterate through your categories and plot the first set of histograms\n",
    "    for idx, (category, cagr_values) in enumerate(cagr_by_category.items(), 1):\n",
    "        plt.subplot(2, 2, idx)\n",
    "        \n",
    "        # Calculate mean, median, variance, and probability\n",
    "        mean_cagr = np.mean(cagr_values)\n",
    "        median_cagr = np.median(cagr_values)\n",
    "        variance_cagr = np.var(cagr_values)\n",
    "        probability_above_20 = sum(c > 20 for c in cagr_values) / len(cagr_values) if cagr_values else 0\n",
    "\n",
    "        # Plot the histogram\n",
    "        n, bins, patches = plt.hist(cagr_values, bins=30, color='skyblue', edgecolor='black')\n",
    "\n",
    "        # Annotate counts on top of each bar\n",
    "        for i in range(len(patches)):\n",
    "            count = int(n[i])  # Get the count for each bar\n",
    "            if count > 0:  # Only annotate bars with counts > 0\n",
    "                plt.text(patches[i].get_x() + patches[i].get_width() / 2, \n",
    "                         n[i], \n",
    "                         str(count), \n",
    "                         ha='center', \n",
    "                         va='bottom', \n",
    "                         fontsize=8, \n",
    "                         color='black')\n",
    "\n",
    "        # Add annotations for statistics in the top-right corner\n",
    "        plt.axvline(mean_cagr, color='red', linestyle='dashed', linewidth=1)\n",
    "        plt.axvline(median_cagr, color='green', linestyle='dashed', linewidth=1)\n",
    "\n",
    "        plt.xlabel('CAGR (%)')\n",
    "        plt.ylabel('Count')\n",
    "        plt.title(f'CAGR Distribution for {category}')\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Annotate statistics\n",
    "        plt.text(0.95, 0.95, f'Mean: {mean_cagr:.2f}%', ha='right', va='top', transform=plt.gca().transAxes, color=\"red\")\n",
    "        plt.text(0.95, 0.90, f'Median: {median_cagr:.2f}%', ha='right', va='top', transform=plt.gca().transAxes, color=\"green\")\n",
    "        plt.text(0.95, 0.85, f'Variance: {variance_cagr:.2f}', ha='right', va='top', transform=plt.gca().transAxes, color=\"blue\")\n",
    "        plt.text(0.95, 0.80, f'P(CAGR > 20%): {probability_above_20:.2%}', ha='right', va='top', transform=plt.gca().transAxes, color=\"purple\")\n",
    "\n",
    "        plt.tight_layout()  # Adjust layout to prevent overlap\n",
    "        \n",
    "        # Save the plot to the images folder\n",
    "        plot_filename = f\"{images_dir}/cagr_distribution_{category.replace(' ', '_')}.png\"\n",
    "        plt.savefig(plot_filename, format=\"png\")\n",
    "\n",
    "        # Write analysis text to README.md\n",
    "        readme.write(f\"## {category} - CAGR Distribution\\n\")\n",
    "        readme.write(f\"Mean CAGR: {mean_cagr:.2f}%\\n\")\n",
    "        readme.write(f\"Median CAGR: {median_cagr:.2f}%\\n\")\n",
    "        readme.write(f\"Variance: {variance_cagr:.2f}\\n\")\n",
    "        readme.write(f\"Probability of CAGR > 20%: {probability_above_20:.2%}\\n\\n\")\n",
    "        readme.write(f\"![CAGR Distribution for {category}](./{plot_filename})\\n\\n\")\n",
    "        \n",
    "        # Clear the plot to avoid overlapping between subplots\n",
    "        plt.clf()\n",
    "\n",
    "    print(f\"Results and plots have been written to {output_file} and images are stored in the '{images_dir}' folder.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e349ee6b-393a-4cf0-bc12-2bf796ef67e4",
   "metadata": {},
   "source": [
    "##### With passing age of the IPO, it is getting tougher to pick the stocks which generates a base 20% CAGR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a66488-8a09-48cc-ae59-b2ca895ccc63",
   "metadata": {},
   "source": [
    "#### Get all the top promisers in each one of the buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3e2e770-e7f7-4e2e-92fd-3bc5901ba21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Table for >0 and <= 1 years with CAGR(>=20%): 22/77 (28.57%)\n",
      "================================================================================\n",
      "QUADFUTURE           Quadrant Future Tek Limited                        33842.54\n",
      "SAGILITY             Sagility India Limited                             3205.94\n",
      "BLACKBUCK            Zinka Logistics Solutions Limited                  576.71\n",
      "KRN                  KRN Heat Exchanger and Refrigeration Limited       446.22\n",
      "EIEL                 Enviro Infra Engineers Limited                     235.09\n",
      "IGIL                 International Gemmological Institute (India) Limited 193.46\n",
      "EPACK                EPACK Durable Limited                              137.60\n",
      "GARUDA               Garuda Construction and Engineering Limited        115.57\n",
      "JGCHEM               J G Chemicals Limited                              105.64\n",
      "AWFIS                Awfis Space Solutions Limited                      105.39\n",
      "INTERARCH            Interarch Building Products Limited                85.82\n",
      "BHARTIHEXA           Bharti Hexacom Limited                             84.21\n",
      "PREMIERENE           Premier Energies Limited                           55.89\n",
      "ABDL                 Allied Blenders and Distillers Limited             46.60\n",
      "NIVABUPA             Niva Bupa Health Insurance Company Limited         36.54\n",
      "SGLTL                Standard Glass Lining Technology Limited           30.65\n",
      "DBEIL                Deepak Builders & Engineers India Limited          29.76\n",
      "BANSALWIRE           Bansal Wire Industries Limited                     25.61\n",
      "AADHARHFC            Aadhar Housing Finance Limited                     24.95\n",
      "KRONOX               Kronox Lab SciencesLimited                         23.88\n",
      "INDGN                Indegene Limited                                   22.67\n",
      "TBOTEK               TBO Tek Limited                                    20.15\n",
      "\n",
      "================================================================================\n",
      "Table for >1 and <= 2 years with CAGR(>=20%): 22/49 (44.90%)\n",
      "================================================================================\n",
      "JYOTICNC             Jyoti CNC Automation Limited                       154.17\n",
      "ZAGGLE               Zaggle Prepaid Ocean Services Limited              124.00\n",
      "EMSLIMITED           EMS Limited                                        116.21\n",
      "AZAD                 Azad Engineering Limited                           111.92\n",
      "SIGNATURE            Signatureglobal (India) Limited                    94.74\n",
      "INNOVACAP            Innova Captab Limited                              77.63\n",
      "CONCORDBIO           Concord Biotech Limited                            74.37\n",
      "SENCO                Senco Gold Limited                                 69.04\n",
      "DOMS                 DOMS Industries Limited                            62.49\n",
      "NETWEB               Netweb Technologies India Limited                  60.31\n",
      "JSWINFRA             JSW Infrastructure Limited                         52.43\n",
      "VPRPL                Vishnu Prakash R Punglia Limited                   46.39\n",
      "MANKIND              Mankind Pharma Limited                             39.19\n",
      "AVALON               Avalon Technologies Limited                        36.72\n",
      "SURAJEST             Suraj Estate Developers Limited                    34.88\n",
      "ASKAUTOLTD           ASK Automotive Limited                             33.39\n",
      "AEROFLEX             Aeroflex Industries Limited                        33.23\n",
      "BLUEJET              Blue Jet Healthcare Limited                        32.16\n",
      "JLHL                 Jupiter Life Line Hospitals Limited                29.31\n",
      "MEDIASSIST           Medi Assist Healthcare Services Limited            22.85\n",
      "SAMHI                SAMHI Hotels Limited                               22.39\n",
      "YATHARTH             Yatharth Hospital and Trauma Care Services Limited 21.24\n",
      "\n",
      "================================================================================\n",
      "Table for >2 and <= 3 years with CAGR(>=20%): 12/34 (35.29%)\n",
      "================================================================================\n",
      "KAYNES               Kaynes Technology India Limited                    162.62\n",
      "PRUDENT              Prudent Corporate Advisory Services Limited        78.78\n",
      "KFINTECH             KFin Technologies Limited                          75.32\n",
      "VENUSPIPES           Venus Pipes and Tubes Limited                      67.09\n",
      "EMUDHRA              eMudhra Limited                                    64.18\n",
      "INOXGREEN            Inox Green Energy Services Limited                 56.70\n",
      "RAINBOW              Rainbow Childrens Medicare Limited                 53.87\n",
      "ETHOSLTD             ETHOS Limited                                      53.70\n",
      "MEDANTA              Global Health Limited                              52.54\n",
      "PARADEEP             Paradeep Phosphates Limited                        45.39\n",
      "BIKAJI               Bikaji Foods International Limited                 42.30\n",
      "EMIL                 Electronics Mart India Limited                     28.96\n",
      "\n",
      "================================================================================\n",
      "Table for >3 and <= 4 years with CAGR(>=20%): 15/57 (26.32%)\n",
      "================================================================================\n",
      "ANANDRATHI           Anand Rathi Wealth Limited                         80.23\n",
      "KALYANKJIL           Kalyan Jewellers India Limited                     60.29\n",
      "DATAPATTNS           Data Patterns (India) Limited                      39.45\n",
      "CRAFTSMAN            Craftsman Automation Limited                       35.44\n",
      "METROBRAND           Metro Brands Limited                               34.41\n",
      "RAILTEL              RailTel Corporation of India Limited               34.27\n",
      "WINDLAS              Windlas Biotech Limited                            28.74\n",
      "RATEGAIN             RateGain Travel Technologies Limited               27.42\n",
      "TEGA                 Tega Industries Limited                            26.95\n",
      "LODHA                Macrotech Developers Limited                       25.65\n",
      "AMIORG               Ami Organics Limited                               25.09\n",
      "SJS                  S J S Enterprises Limited                          23.06\n",
      "SHYAMMETL            Shyam Metalics and Energy Limited                  22.36\n",
      "CMSINFO              CMS Info Systems Limited                           21.38\n",
      "SUPRIYA              Supriya Lifescience Limited                        20.90\n",
      "\n",
      "================================================================================\n",
      "Table for >4 and <= 5 years with CAGR(>=20%): 5/15 (33.33%)\n",
      "================================================================================\n",
      "MAZDOCK              Mazagon Dock Shipbuilders Limited                  83.17\n",
      "ANGELBRKG            Angel Broking Limited                              64.26\n",
      "CAMS                 Computer Age Management Services Limited           28.34\n",
      "BECTORFOOD           Mrs. Bectors Food Specialities Limited             25.16\n",
      "UTIAMC               UTI Asset Management Company Limited               23.41\n",
      "\n",
      "================================================================================\n",
      "Table for >5 and <= 6 years with CAGR(>=20%): 3/12 (25.00%)\n",
      "================================================================================\n",
      "RVNL                 Rail Vikas Nigam Limited                           69.90\n",
      "POLYCAB              Polycab India Limited                              46.38\n",
      "MSTCLTD              MSTC Limited                                       33.71\n",
      "\n",
      "================================================================================\n",
      "Table for >6 and <= 7 years with CAGR(>=20%): 6/23 (26.09%)\n",
      "================================================================================\n",
      "GRSE                 Garden Reach Shipbuilders & Engineers Limited      53.59\n",
      "FINEORG              Fine Organic Industries Limited                    28.92\n",
      "AMBER                Amber Enterprises India Limited                    27.98\n",
      "HGINFRA              H.G.Infra Engineering Limited                      24.88\n",
      "NEWGEN               Newgen Software Technologies Limited               24.62\n",
      "MIDHANI              Mishra Dhatu Nigam Limited                         20.50\n",
      "\n",
      "================================================================================\n",
      "Table for >7 and <= 8 years with CAGR(>=20%): 4/32 (12.50%)\n",
      "================================================================================\n",
      "CDSL                 Central Depository Services (India) Limited        25.95\n",
      "DIXON                Dixon Technologies (India) Limited                 25.75\n",
      "DMART                Avenue Supermarts Limited                          24.49\n",
      "BSE                  BSE Limited                                        23.72\n",
      "\n",
      "================================================================================\n",
      "Table for >8 and <= 9 years with CAGR(>=20%): 2/24 (8.33%)\n",
      "================================================================================\n",
      "LTI                  Larsen & Toubro Infotech Limited                   28.76\n",
      "LTTS                 L&T Technology Services Limited                    24.58\n",
      "\n",
      "================================================================================\n",
      "Table for >9 and <= 10 years with CAGR(>=20%): 1/2 (50.00%)\n",
      "================================================================================\n",
      "ITI                  ITI Limited FPO - Issue Withdrawn                  34.02\n",
      "Results have been written to README.md.\n"
     ]
    }
   ],
   "source": [
    "# Path to the directory containing CSV files\n",
    "data_dir = \"data/\"\n",
    "metadata_file = \"IPO_16To25.csv\"\n",
    "\n",
    "# Initialize dictionary for categories and total stock counts\n",
    "stocks_by_category = {f\">{i} and <= {i + 1} years\": [] for i in range(10)}\n",
    "stocks_by_category[\">0 and <= 1 years\"] = []\n",
    "total_stocks_by_category = {f\">{i} and <= {i + 1} years\": 0 for i in range(10)}\n",
    "total_stocks_by_category[\">0 and <= 1 years\"] = 0\n",
    "\n",
    "# Load IPO metadata to map symbols to company names\n",
    "metadata_df = pd.read_csv(metadata_file, delimiter=\",\")\n",
    "symbol_to_name = dict(zip(metadata_df[\"Symbol\"], metadata_df[\"COMPANY NAME\"]))\n",
    "\n",
    "# Loop through all files in the \"data\" directory\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Extract stock symbol from the filename (excluding the .csv extension)\n",
    "        stock_symbol = filename.replace(\".csv\", \"\")\n",
    "        company_name = symbol_to_name.get(stock_symbol, \"Unknown Company\")\n",
    "        \n",
    "        # Load the stock data from the CSV file\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Parse the oldest and latest dates\n",
    "        oldest_date = datetime.strptime(df['DATE'].iloc[-1], \"%Y-%m-%d\")\n",
    "        latest_date = datetime.strptime(df['DATE'].iloc[0], \"%Y-%m-%d\")\n",
    "        \n",
    "        # Calculate the number of years between the oldest and latest date\n",
    "        years_diff = (latest_date - oldest_date).days / 365.25\n",
    "        \n",
    "        # Calculate CAGR\n",
    "        oldest_close = df['CLOSE'].iloc[-1]  # Oldest price (last row)\n",
    "        latest_close = df['CLOSE'].iloc[0]  # Latest price (first row)\n",
    "        if years_diff > 0:\n",
    "            cagr = ((latest_close / oldest_close) ** (1 / years_diff)) - 1\n",
    "            cagr *= 100  # Convert to percentage\n",
    "            \n",
    "            # Determine the category and increment the total stock count\n",
    "            if years_diff <= 1:\n",
    "                category_key = \">0 and <= 1 years\"\n",
    "            else:\n",
    "                for i in range(1, 10):\n",
    "                    if i < years_diff <= i + 1:\n",
    "                        category_key = f\">{i} and <= {i + 1} years\"\n",
    "                        break\n",
    "            \n",
    "            total_stocks_by_category[category_key] += 1  # Increment total count\n",
    "            \n",
    "            # Assign the stock to the appropriate year category if CAGR >= 20%\n",
    "            if cagr >= 20:\n",
    "                stocks_by_category[category_key].append((stock_symbol, company_name, cagr))\n",
    "\n",
    "# Open the README.md file in write mode\n",
    "with open(output_file, \"a\") as readme:\n",
    "    readme.write(\"# Stock Performance Analysis\\n\")\n",
    "    readme.write(\"This report categorizes stocks by IPO age and displays those with a CAGR of >=20%.\\n\\n\")\n",
    "\n",
    "    # Print tables for each category with success percentage and company names\n",
    "    for category, stocks in stocks_by_category.items():\n",
    "        total_count = total_stocks_by_category[category]\n",
    "        success_count = len(stocks)\n",
    "        success_percentage = (success_count / total_count) * 100 if total_count > 0 else 0\n",
    "\n",
    "        # Print to the console\n",
    "        print(f\"\\n{'='*80}\\nTable for {category} with CAGR(>=20%): {success_count}/{total_count} ({success_percentage:.2f}%)\\n{'='*80}\")\n",
    "        \n",
    "        # Write to README.md\n",
    "        readme.write(f\"## {category}\\n\")\n",
    "        readme.write(f\"Total Stocks in Category: {total_count}\\n\")\n",
    "        readme.write(f\"Success Rate (CAGR >= 20%): {success_percentage:.2f}%\\n\\n\")\n",
    "\n",
    "        if stocks:\n",
    "            # Write table header to README.md\n",
    "            readme.write(\"| Symbol               | Company Name                                         | CAGR (%) |\\n\")\n",
    "            readme.write(\"|----------------------|-----------------------------------------------------|----------|\\n\")\n",
    "\n",
    "            # Print the table content to the console and write it to README.md\n",
    "            for symbol, name, cagr in sorted(stocks, key=lambda x: x[2], reverse=True):\n",
    "                print(f\"{symbol:<20} {name:<50} {cagr:.2f}\")\n",
    "                # Append to the README.md file\n",
    "                readme.write(f\"| {symbol:<20} | {name:<50} | {cagr:.2f} |\\n\")\n",
    "        else:\n",
    "            print(\"No stocks in this category with CAGR >= 20%.\")\n",
    "            readme.write(\"No stocks in this category with CAGR >= 20%.\\n\")\n",
    "        \n",
    "        readme.write(\"\\n---\\n\\n\")\n",
    "\n",
    "print(f\"Results have been written to {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd097f59-e086-41db-a633-68e75b2c8734",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
